---
title: 关于熵的学习
date: 2023-09-20 15:00:00
tags: Machine Learnig
---

# 关于熵的学习

> 学习完扩散模型的推导肯定需要学习如何去训练一个扩散模型，它的先导知识需要了解熵的概念。这个概念连着之前学的哈夫曼编码和哈夫曼树就不难理解。

学习链接：[知乎](https://zhuanlan.zhihu.com/p/501100833)

## 1 熵 $entropy$

数学定义：$H(x)=-\Sigma_xp(x)logp(x)=\Sigma_x\frac{1}{p(x)}$

$H(x)$为随机变量$x$代表的平均信息量，也可以理解为从概率分布$P$中获得一个样本$x$的平均信息量。

## 2 交叉熵 $cross-entropy$

数学定义：$H(p,q)=-\Sigma_xp(x)logq(x)$

此时，$p(x)$为真实值，$q(x)$为预测值。$logq(X)$为信息量。

> 上述公式，当$log$取以2为底的时候，单位为bit。

## 3 举例解释

假设，一个气象预报站预测天气，我们已知天气一共存在八种，它们的概率分别是如下：

|天气|1|2|3|4|5|6|7|8|
|-|-|-|-|-|-|-|-|-|
|概率|35%|35%|10%|10%|4%|4%|1%|1%|

那么气象站给用户发布的熵（理想状态）为：

$$H(x)=-({0.35}\log_{2}{0.35}+{0.1}\log_{2}{0.1}+{0.04}\log_{2}{0.04}+{0.01}\log_{2}{0.01})\times2 \approx 2.23 bit$$

___

假设气象站统一设置了3位长度的编码，此时气象站给用户发布的交叉熵为：$3bit$，不难看出编码存在很大的优化空间。

此时气象站对编码做出了如下优化：

|天气|1|2|3|4|5|6|7|8|
|-|-|-|-|-|-|-|-|-|
|概率|35%|35%|10%|10%|4%|4%|1%|1%|
|编码|00|01|100|101|1100|1101|11100|11101|

此时气象站给用户发布的交叉熵为：

$$H(p,q)=-({0.35}\times2+{0.1}\times3+{0.04}\times4+{0.01}\times5)\times2 = 2.42bit$$

可见交叉熵相较之前$3bit$更接近熵，也就是说改善很多。

___

此时，我们修改不同天气的发生概率但不修改编码规律，如下：

|天气|1|2|3|4|5|6|7|8|
|-|-|-|-|-|-|-|-|-|
|概率|1%|1%|4%|4%|10%|10%|35%|35%|
|编码|00|01|100|101|1100|1101|11100|11101|

那么此时气象站给用户发布的交叉熵为：

$$H(p,q)=-({0.35}\times5+{0.1}\times4+{0.04}\times3+{0.01}\times2)\times2 = 4.58bit$$

此时，信息传输的效率就很差，存在很大一部分冗余。

## 4 理解

利用哈夫曼编码进行理解，在哈夫曼编码中追求的是：出现频率越高的字母，编码长度越短，以追求更短的编码更多的信息。

换成概率去理解，也就是：出现概率越高的事件，用更短的编码，以追求更高的平均信息量。但现实生活的概率不可能预估准确，因此真实值与预估值之间存在一个差值，也就是交叉熵与熵之间会存在一个差值，也就是KL散度（$relative-entropy\ a.k.a.\ KL\ divergence$）。

## 5 KL散度 $relative-entropy\ a.k.a.\ KL\ divergence$

数学定义：$D_{KL}(p||q)=H(p,q)-H(p)$

即在如下情况下：

|天气|1|2|3|4|5|6|7|8|
|-|-|-|-|-|-|-|-|-|
|概率|1%|1%|4%|4%|10%|10%|35%|35%|
|编码|00|01|100|101|1100|1101|11100|11101|

$D_{KL}(p||q)=H(p,q)-H(p) = 4.58-2.23=2.35$

## 6 在机器学习分类问题中的理解

因此，当在机器学习的分类问题中，假设有如下结果：

|天气|1|2|3|4|5|6|7|8|
|-|-|-|-|-|-|-|-|-|
|真实|0%|0%|0%|0%|0%|0%|0%|100%|
|预测|1%|1%|4%|4%|10%|10%|35%|35%|

此时我们预测的交叉熵为：

$$H(p,q)=-100\%\times\log_{2}{0.35}\approx1.515$$

___


若我们预测值与真实值一样时

|天气|1|2|3|4|5|6|7|8|
|-|-|-|-|-|-|-|-|-|
|真实|0%|0%|0%|0%|0%|0%|0%|100%|
|概率|0%|0%|0%|0%|0%|0%|0%|100%|

此时我们预测的交叉熵为：

$$H(p,q)=-100\%\times\log_{2}{1}=0$$

{% note info %}
由此，解释了为什么可以将交叉熵作为$loss$函数，因为在分类问题中，最小化交叉熵就是在使得交叉熵逼近熵，也就是使得预测分布更接近真实分布。
{% endnote %}